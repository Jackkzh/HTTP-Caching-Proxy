Danger Log

-------------------------------------------------------------------
-------------------------------------------------------------------
In our cache strategy, we ignore the cache-control options in request 
while only consider those in response header.

If there is no cache-control in response header, we will set the page 
to be uncacheable, which might be in conflict with the cache-control 
in actual request header.

-------------------------------------------------------------------
-------------------------------------------------------------------
In get request, when the reponse status is 301 with a new Location 
to be a https(connect) request, our proxy would not send the request
 to the new Location and redirect. 

Instead, it would send the reponse to browser, which evantually caused 
a timeout.

-------------------------------------------------------------------
-------------------------------------------------------------------
This is not necessarily a bug. The way we handle request in 
multi-threading is to create a new thread with a copy of the server 
instance passed as a parameter in main.cpp, instead of passing a 
pointer to a smaller data structure that can handle the request.

While it caused no bug, it requires a lot of memory(a quite amount 
of which is uncessary) and might be a potential problem with respect 
to the performance and efficiency.


-------------------------------------------------------------------
-------------------------------------------------------------------
In the strategy of handling GET request: we use vector<char> to receive 
some of the incoming packets, to accommodate incoming packets of varying
 sizes.

However, in our random testing, there are times when vector<char> lead to 
invalid memory access, which we have not solved yet.

As a result, we use char[] in ther other cases, which is not as flexible 
as vector<char> but is more stable. 

-------------------------------------------------------------------
-------------------------------------------------------------------
This Web link: 
http://www.httpwatch.com/httpgallery/chunked/chunkedimage.aspx

we cannot show it successfully. Finally, we found that, the request
it give us has:
"Expires: -1"

However, we only default this value to be a legal time as before.
"-1" means we should not cache it. 

Besides this, we also add "empty check" before parsing, if empty,
we make our "bool isBadRequest" to be true.

In the same way, we also set our "bool isBadGateway".

-------------------------------------------------------------------
-------------------------------------------------------------------
To receive the complete reponse from server, We attempted to use a
while loop to receive the response using content-length in the header
and the total recv size to determine whether the response is complete. 

However, this method is quite slow and inefficient, compared to recv 
just once and send to browser. The problem of recv only one time is 
that it may not receive the complete response sometimes. We handled 
it by marking this web request as incomplete. In future, we should 
balance the trade-offs between 
efficiency and correctness.

-------------------------------------------------------------------
-------------------------------------------------------------------
we first set boost::regex re("Cache-Control:\\s*(\\S+)"); to parse items
after Cache-Control, but that way will stop when it comes to the first ",".

We changed it to boost::regex re("Cache-Control:\\s*([^\\r\\n]+)");

-------------------------------------------------------------------
-------------------------------------------------------------------
The functions we developed for establishing socket connections rely 
on the low-level socket functions, which is less efficient compared to 
using boost::asio. The latter one can better handle corner cases and be 
more convenient to parse header info.